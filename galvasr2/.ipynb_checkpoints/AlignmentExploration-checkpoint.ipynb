{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/spark-9021c19c-918f-4c21-aae2-edb850cc844b/userFiles-7e5d1cac-0c06-431a-9155-d4392ca81657\n",
      "/development/lingvo-source/galvasr2\n",
      "/install/spark/python/lib/py4j-0.10.9-src.zip\n",
      "/install/spark/python\n",
      "/development/lingvo-source/galvasr2\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python37.zip\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7/lib-dynload\n",
      "\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7/site-packages\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7/site-packages/pyspark-3.0.0-py3.7.egg\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7/site-packages/py4j-0.10.9-py3.7.egg\n",
      "/install/miniconda3/envs/100k-hours-lingvo-3/lib/python3.7/site-packages/IPython/extensions\n",
      "/root/.ipython\n",
      "/development/lingvo-source\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sys.path))\n",
    "sys.path.append(\"/development/lingvo-source\")\n",
    "sys.path.append(\"/development/lingvo-source/galvasr2/align\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignmentExploration.ipynb  language_histogram.py\r\n",
      "BUILD\t\t\t    language_histogram.py~\r\n",
      "BUILD~\t\t\t    launch_pyspark_notebook.sh\r\n",
      "align\t\t\t    launch_pyspark_notebook.sh~\r\n",
      "galvasr_latgen_faster.py~   schema.py\r\n",
      "galvasr_tokenize_words.py~  schema.py~\r\n"
     ]
    }
   ],
   "source": [
    "! ls /development/lingvo-source/galvasr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from galvasr2.align.spark.schemas import ARCHIVE_ORG_SCHEMA\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.format('json').schema(ARCHIVE_ORG_SCHEMA).load(\"../ALL_CAPTIONED_DATA.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- audio_document_id: string (nullable = true)\n",
      " |-- text_document_id: string (nullable = true)\n",
      " |-- text_document_format: string (nullable = true)\n",
      "\n",
      "19615\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-77-9e32b44e1061>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-9e32b44e1061>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "exploded_df = df.withColumn(\"exploded_files\", F.explode(df.files))\n",
    "#exploded_df.printSchema()\n",
    "text_df = exploded_df.select(\n",
    "    exploded_df.identifier,\n",
    "    exploded_df.exploded_files[\"name\"].alias(\"text_document_id\"),\n",
    "    exploded_df.exploded_files.format.alias(\"text_document_format\")).where(\n",
    "    (\n",
    "    (exploded_df.exploded_files.format == 'SubRip') | \n",
    "    (exploded_df.exploded_files.format == 'Web Video Text Tracks') |\n",
    "    (exploded_df.exploded_files.format == \"Closed Caption Text\")\n",
    "    )\n",
    "    &\n",
    "    # isNull() indicates that this file was not derived from any other file\n",
    "    (exploded_df.exploded_files.original.isNull())\n",
    "    # According to https://archive.org/help/derivatives.php, \"Closed Caption Text\" is automatically converted to SubRip format.\n",
    "    # (exploded_df.exploded_files.format == \"Closed Caption Text\")\n",
    ")\n",
    "# \n",
    "#    exploded_df.exploded_files.format == \"Web Video Text Tracks\" | \n",
    "#    exploded_df.exploded_files.format == \"Closed Caption Text\")\n",
    "audio_df = exploded_df.select(exploded_df.identifier,\n",
    "                              exploded_df.exploded_files[\"name\"].alias(\"audio_document_id\")).where(exploded_df.exploded_files.format == 'MP3')\n",
    "\n",
    "joined_df = audio_df.join(text_df, \"identifier\")\n",
    "joined_df.printSchema()\n",
    "print(joined_df.count())\n",
    "return\n",
    "#joined_df = joined_df.select(F.lit(\"archive.org\").alias(\"source\"), \n",
    "#                             joined_df.identifier,\n",
    "#                             joined_df.exploded_files[\"name\"].alias(\"audio_document_id\"),\n",
    "#                             )\n",
    "print(blah.head(10))\n",
    "\n",
    "print(text_df.count())\n",
    "# 19487\n",
    "# 19866 without the isNull() constraint. Rather fishy, isn't it?\n",
    "# text_df.printSchema()\n",
    "print(text_df.where(text_df[\"`exploded_files.format`\"] == \"SubRip\").count())\n",
    "print(text_df.where(text_df[\"`exploded_files.format`\"] == \"Web Video Text Tracks\").count())\n",
    "print(text_df.where(text_df[\"`exploded_files.format`\"] == \"Closed Caption Text\").count())\n",
    "\n",
    "print(audio_df.count())\n",
    "# 15026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- exploded_files.name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(exploded_files.name='10_10_2017_Essex_Junction_Trustees.asr.srt'),\n",
       " Row(exploded_files.name='10_10_2017_Essex_Junction_Trustees.en.vtt'),\n",
       " Row(exploded_files.name='10_10_2017_Essex_Junction_Trustees.es.vtt')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Row(exploded_files.name='10_10_2017_Essex_Junction_Trustees.en.vtt'),\n",
    " Row(exploded_files.name='10_10_2017_Essex_Junction_Trustees.es.vtt'),\n",
    " Row(exploded_files.name='10_11_2017_Burlington_Telecom_Advisory_Board.en.vtt'),\n",
    " Row(exploded_files.name='10_11_2017_Burlington_Telecom_Advisory_Board.es.vtt'),\n",
    " Row(exploded_files.name='10_12_2017_Essex_Town_Planning_Commission.en.vtt'),\n",
    " Row(exploded_files.name='10_12_2017_Essex_Town_Planning_Commission.es.vtt'),\n",
    " Row(exploded_files.name='10_16_2017_Essex_Town_Selectboard.en.vtt'),\n",
    " Row(exploded_files.name='10_16_2017_Essex_Town_Selectboard.es.vtt'),\n",
    " Row(exploded_files.name='10_16_2017_Winooski_City_Council.en.vtt'),\n",
    " Row(exploded_files.name='10_16_2017_Winooski_City_Council.es.vtt')]\n",
    "\"\"\"\n",
    "\n",
    "text_df.printSchema()\n",
    "\n",
    "text_df.filter(text_df[\"`exploded_files.name`\"].contains(\"10_10_2017_Essex_Junction_Trustees\")).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|exploded_files[original]| count|\n",
      "+------------------------+------+\n",
      "|                    null|148577|\n",
      "|    PlanningCommissio...|   502|\n",
      "|    BC-2016-0823-Plan...|   500|\n",
      "|    BC_PlanningCommis...|   462|\n",
      "|    town-meeting-2_05...|   386|\n",
      "|    CC-2016-0509-Gove...|   337|\n",
      "|          hbc070213v.asf|   318|\n",
      "|            Grad2002.mpg|   318|\n",
      "|    hrs13REF2154_1002...|   304|\n",
      "|     BOS_092517-CL19.mpg|   304|\n",
      "|            BOSNov12.mpg|   304|\n",
      "|    mv_senate_proceed...|   304|\n",
      "|    1_2_2018_South_Bu...|   304|\n",
      "|    Richmond_Selectbo...|   304|\n",
      "|    hrs03A_S2118_0901...|   304|\n",
      "|    Town Meeting 0606...|   304|\n",
      "|    Richmond_Selectbd...|   304|\n",
      "|    hrs02A_S_HVC-210_...|   304|\n",
      "|    mv_senate_proceed...|   303|\n",
      "|    mv_house_proceedi...|   303|\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df.groupBy(exploded_df.exploded_files.original).count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'align'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-b442c4439ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#importlib.reload(\"align\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'align'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"identifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text_document_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text_document_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'align'"
     ]
    }
   ],
   "source": [
    "# WEBVTT\n",
    "# Kind: captions\n",
    "# Language: es\n",
    "\n",
    "# Useful WEBVTT metadata. It knows its own language and whether or not these are captions.\n",
    "\n",
    "from galvasr2.align.spark import align # import load_transcripts\n",
    "import importlib\n",
    "#importlib.reload(\"align\")\n",
    "importlib.reload(sys.modules['align'])\n",
    "\n",
    "rows = joined_df.select(\"identifier\", \"text_document_id\", \"text_document_format\").collect()\n",
    "print(\"\\n\".join(str(row) for row in rows[:10]))\n",
    "result = align.load_transcripts(spark, \"gs://the-peoples-speech-west-europe/archive_org/small_dataset\", rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "?pyspark.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
