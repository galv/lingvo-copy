{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.functions import array, array_contains, count, explode, lit, sum\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "#spark.conf.set(\"io.netty.tryReflectionSetAccessible\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"created\", LongType(), True),\n",
    "    StructField(\"d1\", StringType(), True),\n",
    "    StructField(\"d2\", StringType(), True),\n",
    "    StructField(\"dir\", StringType(), True),\n",
    "    StructField(\n",
    "        \"files\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"bitrate\", StringType(), True),\n",
    "                StructField(\"btih\", StringType(), True),\n",
    "                StructField(\"crc32\", StringType(), True),\n",
    "                StructField(\"format\", StringType(), True),\n",
    "                StructField(\"height\", StringType(), True),\n",
    "                StructField(\"length\", StringType(), True),\n",
    "                StructField(\"license\", StringType(), True),\n",
    "                StructField(\"md5\", StringType(), True),\n",
    "                StructField(\"mtime\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"original\", StringType(), True),\n",
    "                StructField(\"rotation\", StringType(), True),\n",
    "                StructField(\"sha1\", StringType(), True),\n",
    "                StructField(\"size\", StringType(), True),\n",
    "                StructField(\"source\", StringType(), True),\n",
    "                StructField(\"title\", StringType(), True),\n",
    "                StructField(\"track\", StringType(), True),\n",
    "                StructField(\"width\", StringType(), True)\n",
    "            ]), True), \n",
    "        True),\n",
    "    StructField(\"files_count\", LongType(), True),\n",
    "    StructField(\"identifier\", StringType(), True),\n",
    "    StructField(\"item_last_updated\", LongType(), True),\n",
    "    StructField(\"item_size\", LongType(), True),\n",
    "    StructField(\n",
    "        \"metadata\",\n",
    "        StructType([\n",
    "            StructField(\"Length\", StringType(), True),\n",
    "            StructField(\"addeddate\", StringType(), True),\n",
    "            StructField(\"adder\", StringType(), True),\n",
    "            StructField(\"aspect_ratio\", StringType(), True),\n",
    "            StructField(\"backup_location\", StringType(), True),\n",
    "            StructField(\"closed_captioning\", StringType(), True),\n",
    "            StructField(\"collection\", ArrayType(StringType(), True), True),\n",
    "            StructField(\"color\", StringType(), True),\n",
    "            StructField(\"contact\", StringType(), True),\n",
    "            StructField(\"coverage\", StringType(), True),\n",
    "            StructField(\"creator\", StringType(), True),\n",
    "            StructField(\"credits\", StringType(), True),\n",
    "            StructField(\"curation\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"director\", StringType(), True),\n",
    "            StructField(\"duration\", StringType(), True),\n",
    "            StructField(\"format\", StringType(), True),\n",
    "            StructField(\"genre\", StringType(), True),\n",
    "            StructField(\"glob\", StringType(), True),\n",
    "            StructField(\"holder\", StringType(), True),\n",
    "            StructField(\"ia_orig__runtime\", StringType(), True),\n",
    "            StructField(\"identifier\", StringType(), True),\n",
    "            StructField(\"identifier-access\", StringType(), True),\n",
    "            StructField(\"identifier-ark\", StringType(), True),\n",
    "            StructField(\"imdb\", StringType(), True),\n",
    "            StructField(\"keywords\", StringType(), True),\n",
    "            StructField(\"language\", StringType(), True),\n",
    "            StructField(\"lcenseurl\", StringType(), True),\n",
    "            StructField(\"license\", StringType(), True),\n",
    "            StructField(\"licenseurl\", StringType(), True),\n",
    "            StructField(\"licensurl\", StringType(), True),\n",
    "            StructField(\"mediatype\", StringType(), True),\n",
    "            StructField(\"noarchivetorrent\", StringType(), True),\n",
    "            StructField(\"ocr\", StringType(), True),\n",
    "            StructField(\"omp-locally-produced\", StringType(), True),\n",
    "            StructField(\"omp-project\", StringType(), True),\n",
    "            StructField(\"own\", StringType(), True),\n",
    "            StructField(\"pbcore-genre\", StringType(), True),\n",
    "            StructField(\"pick\", StringType(), True),\n",
    "            StructField(\"ppi\", StringType(), True),\n",
    "            StructField(\"presenter\", StringType(), True),\n",
    "            StructField(\"producer\", StringType(), True),\n",
    "            StructField(\"publicdate\", StringType(), True),\n",
    "            StructField(\"publisher\", StringType(), True),\n",
    "            StructField(\"release_date\", StringType(), True),\n",
    "            StructField(\"repub_state\", StringType(), True),\n",
    "            StructField(\"resource\", StringType(), True),\n",
    "            StructField(\"runtime\", StringType(), True),\n",
    "            StructField(\"scanner\", StringType(), True),\n",
    "            StructField(\"segments\", StringType(), True),\n",
    "            StructField(\"series\", StringType(), True),\n",
    "            StructField(\"sound\", StringType(), True),\n",
    "            StructField(\"sponsor\", StringType(), True),\n",
    "            StructField(\"subject\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"tv-parental-guidelines\", StringType(), True),\n",
    "            StructField(\"updatedate\", StringType(), True),\n",
    "            StructField(\"updater\", StringType(), True),\n",
    "            StructField(\"upload_application\", StringType(), True),\n",
    "            StructField(\"uploader\", StringType(), True),\n",
    "            StructField(\"vimeo-height\", StringType(), True),\n",
    "            StructField(\"vimeo-id\", StringType(), True),\n",
    "            StructField(\"vimeo-n-entries\", StringType(), True),\n",
    "            StructField(\"vimeo-playlist\", StringType(), True),\n",
    "            StructField(\"vimeo-playlist-index\", StringType(), True),\n",
    "            StructField(\"vimeo-uploader\", StringType(), True),\n",
    "            StructField(\"vimeo-uploader-id\", StringType(), True),\n",
    "            StructField(\"vimeo-view-count\", StringType(), True),\n",
    "            StructField(\"vimeo-webpage-url\", StringType(), True),\n",
    "            StructField(\"vimeo-width\", StringType(), True),\n",
    "            StructField(\"year\", StringType(), True),\n",
    "            StructField(\"youtube-height\", StringType(), True),\n",
    "            StructField(\"youtube-id\", StringType(), True),\n",
    "            StructField(\"youtube-n-entries\", StringType(), True),\n",
    "            StructField(\"youtube-playlist\", StringType(), True),\n",
    "            StructField(\"youtube-playlist-index\", StringType(), True),\n",
    "            StructField(\"youtube-uploader\", StringType(), True),\n",
    "            StructField(\"youtube-uploader-id\", StringType(), True),\n",
    "            StructField(\"youtube-view-count\", StringType(), True),\n",
    "            StructField(\"youtube-webpage-url\", StringType(), True),\n",
    "            StructField(\"youtube-width\", StringType(), True)\n",
    "        ]), True),\n",
    "])\n",
    "\n",
    "# Local copy of gs://the-peoples-speech-west-europe/archive_org/Nov_6_2020/ALL_CAPTIONED_DATA.jsonl.gz\n",
    "#df = spark.read.schema(schema).json(\"/home/ws15dgalvez/lingvo-copy/scripts/archive.org/ALL_CAPTIONED_DATA.jsonl.gz\")\n",
    "df = None\n",
    "df = spark.read.schema(schema).json(\"gs://the-peoples-speech-west-europe/archive_org/Nov_6_2020/ALL_CAPTIONED_DATA.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+\n",
      "|((sum(CAST(metadata.duration AS DOUBLE)) / 60) / 60)|\n",
      "+----------------------------------------------------+\n",
      "|                                  3881.1705555555554|\n",
      "+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of hours. The null values are suspicious...\n",
    "df.select(sum(col(\"metadata.duration\").cast(DoubleType())) / 60 / 60).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred language:\n",
      "[Row(infer_language_func(metadata.description)='en', count=13162), Row(infer_language_func(metadata.description)='', count=3012), Row(infer_language_func(metadata.description)='de', count=80), Row(infer_language_func(metadata.description)='es', count=72), Row(infer_language_func(metadata.description)='fr', count=61), Row(infer_language_func(metadata.description)='fi', count=30), Row(infer_language_func(metadata.description)='nl', count=28), Row(infer_language_func(metadata.description)='la', count=20), Row(infer_language_func(metadata.description)='da', count=17), Row(infer_language_func(metadata.description)='it', count=16)]\n",
      "Declared language:\n",
      "[Row(language='English', count=7776), Row(language=None, count=6956), Row(language='eng', count=1791), Row(language='spanish', count=35), Row(language='english', count=15), Row(language='Mandarin Chinese', count=11), Row(language='Spanish', count=7), Row(language='English (dubbed)', count=6), Row(language='Hmong', count=3), Row(language='Arabic', count=2)]\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "\n",
    "text_series = pd.Series([\"bonjour\", \"Espania\"], dtype=pd.StringDtype())\n",
    "#df = spark.createDataFrame(pd.DataFrame(text_series, columns=[\"text\"]))\n",
    "\n",
    "def infer_language_func(text_column: pd.Series) -> pd.Series:\n",
    "    return text_column.map(lambda string: langid.classify(string)[0] if string else \"\")\n",
    "\n",
    "infer_language = pandas_udf(infer_language_func, returnType=StringType())\n",
    "\n",
    "print(\"Inferred language:\")\n",
    "print(df.select(infer_language(col(\"metadata.description\"))).groupBy(col('`infer_language_func(metadata.description)`')).count().orderBy('count', ascending=False).head(10))\n",
    "print(\"Declared language:\")\n",
    "print(df.select(col(\"metadata.language\")).groupBy(col('language')).count().orderBy('count', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select(sum(explode(col(\"files.length\")).cast(DoubleType())) /60. / 60.).show()\n",
    "\n",
    "files_df = df.select(explode(col(\"files\")).alias(\"file\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: struct (nullable = true)\n",
      " |    |-- bitrate: string (nullable = true)\n",
      " |    |-- btih: string (nullable = true)\n",
      " |    |-- crc32: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- height: string (nullable = true)\n",
      " |    |-- length: string (nullable = true)\n",
      " |    |-- license: string (nullable = true)\n",
      " |    |-- md5: string (nullable = true)\n",
      " |    |-- mtime: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |    |-- rotation: string (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- size: string (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- track: string (nullable = true)\n",
      " |    |-- width: string (nullable = true)\n",
      "\n",
      "+---------------------+\n",
      "|to_date(30:26, mm:ss)|\n",
      "+---------------------+\n",
      "|           1970-01-01|\n",
      "+---------------------+\n",
      "\n",
      "[Row(CAST(to_timestamp(t, mm:ss) AS BIGINT)=630)]\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input 'to_timestamp' expecting {<EOF>, ';'}(line 2, pos 17)\n\n== SQL ==\n\nSELECT CASE WHEN to_timestamp(\"file.length\", 'mm:ss') IS NOT NULL THEN CAST(to_timestamp(\"file.length\", 'mm:ss') AS Long)\n-----------------^^^\n    ELSE CAST (\"file.length\" AS LONG) / 60. / 60.\nFROM files\nWHERE file.length IS NOT NULL\nAND file.source == 'original'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f7f79849b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mWHERE\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mAND\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'original'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \"\"\").show()\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \"\"\"\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmismatched input 'to_timestamp' expecting {<EOF>, ';'}(line 2, pos 17)\n\n== SQL ==\n\nSELECT CASE WHEN to_timestamp(\"file.length\", 'mm:ss') IS NOT NULL THEN CAST(to_timestamp(\"file.length\", 'mm:ss') AS Long)\n-----------------^^^\n    ELSE CAST (\"file.length\" AS LONG) / 60. / 60.\nFROM files\nWHERE file.length IS NOT NULL\nAND file.source == 'original'\n"
     ]
    }
   ],
   "source": [
    "files_df.printSchema()\n",
    "# Why is \"col\" here?\n",
    "#files_df.select(\"file.bitrate\")\n",
    "files_df.createOrReplaceTempView(\"files\")\n",
    "#spark.sql(\"SELECT SUM(CAST(file.length AS DOUBLE)) / 60. / 60. FROM files\").show()\n",
    "#spark.sql(\"SELECT file.length FROM files WHERE file.length is NOT NULL\").show()\n",
    "\n",
    "spark.sql(\"SELECT to_date('30:26', 'mm:ss')\").show()\n",
    "\n",
    "blah_df = spark.createDataFrame([('10:30',)], ['t'])\n",
    "from pyspark.sql.functions import to_date, to_timestamp  # , to_unix_timestamp\n",
    "print(blah_df.select(to_timestamp(blah_df.t, 'mm:ss').cast(LongType())).collect()) # - to_date('00:00', 'mm:ss').alias('date')).show()\n",
    "\n",
    "# https://stackoverflow.com/a/54433013\n",
    "# val timesTwoUDF = spark.udf.register(\"timesTwo\", (x: Int) => x * 2)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT CASE WHEN to_timestamp(\"file.length\", 'mm:ss') IS NOT NULL THEN CAST(to_timestamp(\"file.length\", 'mm:ss') AS Long)\n",
    "    ELSE CAST (\"file.length\" AS LONG) / 60. / 60.\n",
    "FROM files\n",
    "WHERE file.length IS NOT NULL\n",
    "AND file.source == 'original'\n",
    "\"\"\").show()\n",
    "\n",
    "#spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "#spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/ws15dgalvez/the-peoples-speech-d0aa630b119d.json\")\n",
    "\n",
    "\n",
    "\n",
    "#to_date\n",
    "#mm:ss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "?pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#def infer_language_func(text_column: pd.Series) -> pd.Series:\n",
    "#    return text_column.map(lambda string: langid.classify(string)[0] if string else \"\")\n",
    "#infer_language = pandas_udf(infer_language_func, returnType=StringType())\n",
    "\n",
    "def get_audio_data(raw_audio_binary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame(cols=[\"signed_int16_waveform\"])\n",
    "    for row in raw_audio.itertuples():\n",
    "        _, file_type = os.path.splitext(row.path)\n",
    "        file_type = file_type.lstrip(\".\")\n",
    "        # Always output in 16000 Hz\n",
    "        \n",
    "        with NamedTemporaryFile() as fh:\n",
    "            cmd = f'sox -t {fmt} - -t wav --channels 1 --rate 16000 --encoding signed --bits 16 {fh.name}'\n",
    "            p = subprocess.Popen(\n",
    "                shlex.split(cmd),\n",
    "              stdin=subprocess.PIPE,\n",
    "              stdout=subprocess.PIPE,\n",
    "              stderr=subprocess.PIPE)\n",
    "            _, err = p.communicate(input=row.content)\n",
    "            assert p.returncode == 0, err\n",
    "            signed_int16_waveform = fh.read()\n",
    "    return new_df\n",
    "\n",
    "def voice_activity_detection():\n",
    "    pass\n",
    "\n",
    "# Change to */*.mp3 later\n",
    "raw_audio_df = (spark.read.format(\"binaryFile\")\n",
    "                .option(\"pathGlobFilter\", \"*.mp3\")\n",
    "                .load(\"gs://the-peoples-speech-west-europe/archive_org/Nov_6_2020/ALL_CAPTIONED_DATA/bicycle_today_automobile_tomorrow\"))\n",
    "raw_audio_pd = raw_audio_df.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modificationTime</th>\n",
       "      <th>length</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://the-peoples-speech-west-europe/archive_or...</td>\n",
       "      <td>2020-11-07 11:30:50.704</td>\n",
       "      <td>4827136</td>\n",
       "      <td>[73, 68, 51, 3, 0, 128, 0, 0, 6, 57, 67, 79, 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path        modificationTime  \\\n",
       "0  gs://the-peoples-speech-west-europe/archive_or... 2020-11-07 11:30:50.704   \n",
       "\n",
       "    length                                            content  \n",
       "0  4827136  [73, 68, 51, 3, 0, 128, 0, 0, 6, 57, 67, 79, 7...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_audio_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_audio_pd = raw_audio_df.toPandas()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
